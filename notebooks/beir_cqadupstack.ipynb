{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "YaPbEXlHvd_C",
   "metadata": {
    "id": "YaPbEXlHvd_C",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# CQADupStack Collection\n",
    "\n",
    "The [*CQADupStack*](https://github.com/D1Doris/CQADupStack) is \"[a] Benchmark Data Set for Community Question-Answering Research\" [1] that is a part of the [*Benchmarking Information Retrieval (BEIR)*](https://github.com/beir-cellar/beir) collection.\n",
    "\n",
    "CQADupStack contains data from 12 different [*Stackexchange*](https://stackexchange.com/) subforums based on the data dump released on September 26, 2014.\n",
    "\n",
    "Your tasks, reviewed by your colleagues and the course instructors, are the following:\n",
    "\n",
    "TODO\n",
    "\n",
    "\n",
    "1. *Implement a ranked retrieval system*, [1, Chapter 6] which will produce a list of documents from the CQADupStack collection in a descending order of relevance to a query from the CQADupStack collection.\n",
    "2. *Document your code* in accordance with [PEP 257](https://www.python.org/dev/peps/pep-0257/), ideally using [the NumPy style guide](https://numpydoc.readthedocs.io/en/latest/format.html#docstring-standard) as seen in the code from exercises.\n",
    "   *Stick to a consistent coding style* in accordance with [PEP 8](https://www.python.org/dev/peps/pep-0008/).\n",
    "3. *Reach at least XX% mean average precision* [1, Section 8.4] with your system on the CQADupStack collection.\n",
    "4.   _[Upload an .ipynb file](https://is.muni.cz/help/komunikace/spravcesouboru#k_ss_1) with this Jupyter notebook to the homework vault in IS MU._ You MAY also include a brief description of your information retrieval system and a link to an external service such as [Google Colaboratory](https://colab.research.google.com/), [DeepNote](https://deepnote.com/), or [JupyterHub](https://iirhub.cloud.e-infra.cz/).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "[1] Hoogeveen, Doris and Verspoor, Karin M. and Baldwin, Timothy. [*CQADupStack: A Benchmark Data Set for Community Question-Answering Research*](https://dl.acm.org/doi/10.1145/2838931.2838934). ACM, 2015."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "u6zDHKMAvvTt",
   "metadata": {
    "id": "u6zDHKMAvvTt",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Import the utility tools from the git repository.\n",
    "\n",
    "First, we will install [our library](https://github.com/MIR-MU/pv211-utils).\n",
    "\n",
    "It may be necessary to restart the runtime to get the installed packages to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3db5c20f-031f-4d05-abea-4570e80fca34",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3db5c20f-031f-4d05-abea-4570e80fca34",
    "outputId": "b559eea8-922c-4a20-ca14-cc2126e50028",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#! pip install git+https://github.com/MIR-MU/pv211-utils.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "I0_jZfSIwfcv",
   "metadata": {
    "id": "I0_jZfSIwfcv",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Define the necessary classes\n",
    "\n",
    "These will eventually represent the Queries, Documents and Relevance Judgements from the CQADupStack collection.\n",
    "\n",
    "Query and Document consist only of their IDs and bodies.\n",
    "Judgements are also just a Set of Tuples that represent pairs of relevant Document-Query combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3f3f1a8-2a5b-4d3f-9e68-175909efe906",
   "metadata": {
    "id": "d3f3f1a8-2a5b-4d3f-9e68-175909efe906",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pv211_utils.beir.entities import BeirDocumentBase, BeirQueryBase, BeirJudgementBase\n",
    "from typing import Set\n",
    "\n",
    "class Query(BeirQueryBase):\n",
    "    \"\"\"\n",
    "    A processed query form the Beir collection.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    query_id : int\n",
    "        The number\n",
    "    body : str\n",
    "        Text of a query\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, query_id: int, body: str):\n",
    "        super().__init__(query_id, body)\n",
    "\n",
    "class Document(BeirDocumentBase):\n",
    "    \"\"\"\n",
    "    A processed document form the Beir collection.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    document_id : str\n",
    "        A unique identifier of the document.\n",
    "    body : str\n",
    "        The text of the document.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, document_id: str, body: str):\n",
    "        super().__init__(document_id, body)\n",
    "        \n",
    "BeirJudgements = Set[BeirJudgementBase]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HQEFV7C5wtZv",
   "metadata": {
    "id": "HQEFV7C5wtZv",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Define the datasets that are to be used.\n",
    "\n",
    "\n",
    "TODO - Either this or the one bellow\n",
    "\n",
    "RawBeirDataset stores the basic setup \n",
    "- Name of the dataset\n",
    "- Subset(s) to use\n",
    "- Alternative(s) to the subset(s) if they are not available\n",
    "\n",
    "\n",
    "RawBeirDatasets then stores \n",
    "- Common download location\n",
    "- List of RawBeirDataset instances\n",
    "\n",
    "If more than one datasets are used, they will be merged and used as one. This functionality is primarly aimed at the CQADupstack datasets, but shuld work with any other combination as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8f6e32",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Loading the datasets\n",
    "\n",
    "\n",
    "First, we import the RawBeirDataset, RawBeirDatasets classes.\n",
    "\n",
    "The former is used to simply save the name and desired subsets of the selected CQADupStack dataset. All CQADupStack datasets contain only the testing subset out-of-the-box.\n",
    "\n",
    "The latter is necessary for the purpose of using multiple datasets at once. In this example, we combine the Android and Programmers datasets.\n",
    "\n",
    "It is necessary to define the path to the download directory, where the datasets will be stored. If a desired dataset is already present in this selected directory, repeated download will not be necessary.\n",
    "\n",
    "\n",
    "\n",
    "### CQADupStack contains 12 datasets:\n",
    "- Android\n",
    "- English\n",
    "- Gaming\n",
    "- GIS\n",
    "- Mathematica\n",
    "- Physics\n",
    "- Programmers\n",
    "- Stats\n",
    "- TeX\n",
    "- Unix\n",
    "- Webmasters\n",
    "- WordPress\n",
    "\n",
    "These are represented by theirs lowercase names.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f663c408-3cf8-4ad5-b6ca-8ab108c553df",
   "metadata": {
    "id": "f663c408-3cf8-4ad5-b6ca-8ab108c553df",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pv211_utils.beir.entities import RawBeirDataset, RawBeirDatasets\n",
    "\n",
    "\n",
    "android = RawBeirDataset( \"android\", test = True)\n",
    "english = RawBeirDataset(\"english\", test = True)\n",
    "gaming = RawBeirDataset(\"gaming\", test = True)\n",
    "gis = RawBeirDataset(\"gis\", test = True)\n",
    "mathematica = RawBeirDataset(\"mathematica\", test = True)\n",
    "physics = RawBeirDataset(\"physics\", test = True)\n",
    "programmers = RawBeirDataset(\"programmers\", test = True)\n",
    "stats = RawBeirDataset(\"stats\", test = True)\n",
    "tex = RawBeirDataset(\"tex\", test = True)\n",
    "unix = RawBeirDataset(\"unix\",test = True)\n",
    "webmasters = RawBeirDataset(\"webmasters\", test = True)\n",
    "wordpress = RawBeirDataset(\"wordpress\", test = True)\n",
    "\n",
    "#programmers = RawBeirDataset(\"programmers\",test = True)\n",
    "#android = RawBeirDataset(\"android\",test = True)\n",
    "download_location = \"datasets\"\n",
    "desired_datasets = RawBeirDatasets( datasets=[android,english,gaming,gis,mathematica,physics,programmers,stats,tex,unix,webmasters,wordpress],download_location=download_location)\n",
    "\n",
    "# If download_location is set to None the data should not re-download, instead it will load from default location\n",
    "# desired_datasets = RawBeirDatasets([android,english,gaming,gis,mathematica,physics,programmers,stats,tex,unix,webmasters,wordpress]\", None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jo_45suJzxkX",
   "metadata": {
    "id": "jo_45suJzxkX",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Load and split raw data\n",
    "Once we have all the desired datasets defined we can load them, load_BEIR_datasets downloads (if necessary), loads, and prepares the raw data from the selected datasets.\n",
    "\n",
    "It returns three values:\n",
    "raw_train_data, raw_dev_data, raw_test_data,\n",
    "but as the train and dev subsets are not present in these datasets, these can be ignored.\n",
    "\n",
    "To get the train and dev(validation) subsets, we use the split_BEIR_dataset to split the original test data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f5a0b15-d2ee-405e-81be-64c627a02454",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8f5a0b15-d2ee-405e-81be-64c627a02454",
    "outputId": "dd471733-f0eb-43a1-f4ca-439a84caf99c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22998/22998 [00:00<00:00, 239155.16it/s]\n",
      "100%|██████████| 40221/40221 [00:00<00:00, 225795.58it/s]\n",
      "100%|██████████| 45301/45301 [00:00<00:00, 250471.15it/s]\n",
      "100%|██████████| 37637/37637 [00:00<00:00, 200424.84it/s]\n",
      "100%|██████████| 16705/16705 [00:00<00:00, 185891.49it/s]\n",
      "100%|██████████| 38316/38316 [00:00<00:00, 211597.24it/s]\n",
      "100%|██████████| 32176/32176 [00:00<00:00, 190718.35it/s]\n",
      "100%|██████████| 42269/42269 [00:00<00:00, 199153.28it/s]\n",
      "100%|██████████| 68184/68184 [00:00<00:00, 163860.88it/s]\n",
      "100%|██████████| 47382/47382 [00:00<00:00, 205791.73it/s]\n",
      "100%|██████████| 17405/17405 [00:00<00:00, 235110.66it/s]\n",
      "100%|██████████| 48605/48605 [00:00<00:00, 195728.50it/s]\n"
     ]
    }
   ],
   "source": [
    "from pv211_utils.beir.loader import load_beir_datasets, split_beir_dataset\n",
    "\n",
    "\n",
    "_, _, raw_test_data = load_beir_datasets(desired_datasets)\n",
    "# Leave 90% for training and 10% for validation and testing\n",
    "raw_train_data, raw_test_data = split_beir_dataset(raw_test_data, split_factor=0.9)\n",
    "# Subsequently split this into 5% for validation and 5% for testing\n",
    "raw_dev_data, raw_train_data = split_beir_dataset(raw_train_data, split_factor=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "knotOm8k1IdG",
   "metadata": {
    "id": "knotOm8k1IdG",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### The loaded raw data consists of three parts.\n",
    "- corpus (the set of documents)\n",
    "- queries (the search terms)\n",
    "- qrels (the relevance judgements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d90f525-359f-4a4e-9937-e04665439268",
   "metadata": {
    "id": "0d90f525-359f-4a4e-9937-e04665439268",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "raw_corpus_test = list(raw_test_data)[0]\n",
    "raw_queries_test = list(raw_test_data)[1]\n",
    "raw_qrels_test = list(raw_test_data)[2]\n",
    "\n",
    "raw_corpus_train = list(raw_train_data)[0]\n",
    "raw_queries_train = list(raw_train_data)[1]\n",
    "raw_qrels_train = list(raw_train_data)[2]\n",
    "\n",
    "raw_corpus_dev = list(raw_dev_data)[0]\n",
    "raw_queries_dev = list(raw_dev_data)[1]\n",
    "raw_qrels_dev = list(raw_dev_data)[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8zj4JqY41rDS",
   "metadata": {
    "id": "8zj4JqY41rDS",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Process the loaded data\n",
    "\n",
    "In order to use the loaded data it is necessary to process it.\n",
    "\n",
    "load_X functions return a list of the processed data.\n",
    "\n",
    "If desired or necessary it is possible to limit the number of used queries.\n",
    "(to speed up the evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21048042-693b-44c5-be92-63ba12783ef0",
   "metadata": {
    "id": "21048042-693b-44c5-be92-63ba12783ef0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pv211_utils.beir.loader import load_documents,load_queries,load_judgements\n",
    "\n",
    "# generally the same set of documents is used for train, dev, and test - no need to store it multiple times\n",
    "documents = load_documents(raw_corpus_test)\n",
    "max_test_queries = None # for HotpotQA I suggest for example just 200 test queries, None == use all avaialbe \n",
    "test_queries = load_queries(raw_queries_test,max_test_queries)\n",
    "test_judgements = load_judgements(test_queries,documents,raw_qrels_test)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "H6oU2MTN54pF",
   "metadata": {
    "id": "H6oU2MTN54pF",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Implement the Information Retrieval system\n",
    "\n",
    "Next, we will define a class named `IRSystem` that will represent your information retrieval system. Your class must define a method name `search` that takes a query and returns documents in descending order of relevance to the query.\n",
    "\n",
    "This example returns documents in a decreasing order according to\n",
    "a [*Okapi BestMatch25*](https://en.wikipedia.org/wiki/Okapi_BM25) similarity score between the documents and the given query.\n",
    "\n",
    "It also allows for the use of a [*re-ranking*](https://developers.google.com/machine-learning/recommendation/dnn/re-ranking) function, which takes the selected top k results from the base function and reorders them to achieve better results.\n",
    "\n",
    "You can use this example as the basis for your own implementation of an Information Retrieval System. Experiment with better suited preprocessing options. Try different base and re-ranking functions. Play around with the various hyperparameters, either by hand or you can use the dev subset to help you find the best setup. And of course, you can scratch this entire piece of code and make something completely new.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c30058ea-71b9-4e15-8105-4e427445ab06",
   "metadata": {
    "id": "c30058ea-71b9-4e15-8105-4e427445ab06",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "\n",
    "from multiprocessing import get_context, Pool\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from pv211_utils.irsystem import IRSystemBase \n",
    "\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.summarization import bm25\n",
    "\n",
    "\n",
    "\n",
    "class IRSystem(IRSystemBase):\n",
    "    \"\"\"\n",
    "    A system that returns documents ordered based on the\n",
    "    Okapi BestMach25 score.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        document_bodies = [simple_preprocess(doc.body) for doc in documents.values()]\n",
    "        document_bodies = tqdm(document_bodies, desc='Preprocessing documents', total=len(documents))\n",
    "        index_to_document = dict(enumerate(documents.values()))\n",
    "        self.index_to_document = index_to_document\n",
    "        bm25_model = bm25.BM25(document_bodies)\n",
    "        self.bm25_model = bm25_model\n",
    "\n",
    "\n",
    "    def search(self, query: Query) -> Iterable[Document]:\n",
    "        query_doc = simple_preprocess(query.body)\n",
    "        similarities = enumerate(self.bm25_model.get_scores(query_doc,\n",
    "                                                            sum(map(lambda k: float(self.bm25_model.idf[k]), self.bm25_model.idf.keys())) / len(self.bm25_model.idf.keys())))\n",
    "        similarities = sorted(similarities, key=lambda item: item[1], reverse=True)\n",
    "        ranked_documents = list(self.index_to_document[i] for i,_ in similarities)\n",
    "\n",
    "        for doc in ranked_documents:\n",
    "            yield doc\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EiuNlaI76EPe",
   "metadata": {
    "id": "EiuNlaI76EPe",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Evaluate the system on a given dataset\n",
    "\n",
    "Lastly, we will evaluate the IR system using the [Mean Average Precision](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Mean_average_precision) (MAP).\n",
    "\n",
    "Set the rerank_first_k parameter to a nonzero number such as a 100 to use the reranking function on the top k results of every search.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e66f8a32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.005597114562988281,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Preprocessing documents",
       "rate": null,
       "total": 457159,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feadfb84ad1445d792601c4501715281",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preprocessing documents:   0%|          | 0/457159 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.004961252212524414,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Querying the system",
       "rate": null,
       "total": 11831,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60904cc7d1294fc08a40b9014b0c8088",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Querying the system:   0%|          | 0/11831 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'gensim.summarization.bm25' has no attribute 'idf'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [10], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m system \u001b[39m=\u001b[39m IRSystem()\n\u001b[1;32m      7\u001b[0m evaluation \u001b[39m=\u001b[39m BeirEvaluation(system, test_judgements, \u001b[39mNone\u001b[39;00m,author_name, num_workers\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m evaluation\u001b[39m.\u001b[39;49mevaluate(tqdm(test_queries\u001b[39m.\u001b[39;49mvalues(), desc\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mQuerying the system\u001b[39;49m\u001b[39m'\u001b[39;49m), submit_result)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.14/lib/python3.9/site-packages/pv211_utils/eval.py:183\u001b[0m, in \u001b[0;36mEvaluationBase.evaluate\u001b[0;34m(self, queries, submit_result)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[39m\"\"\"Evaluates the information retrieval system and provides feedback.\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \n\u001b[1;32m    173\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    180\u001b[0m \n\u001b[1;32m    181\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    182\u001b[0m time_before \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mnow()\n\u001b[0;32m--> 183\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmean_average_precision(queries)\n\u001b[1;32m    184\u001b[0m time_after \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mnow()\n\u001b[1;32m    185\u001b[0m map_score \u001b[39m=\u001b[39m result \u001b[39m*\u001b[39m \u001b[39m100.0\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.14/lib/python3.9/site-packages/pv211_utils/eval.py:123\u001b[0m, in \u001b[0;36mEvaluationBase.mean_average_precision\u001b[0;34m(self, queries)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m_CURRENT_INSTANCE \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\n\u001b[1;32m    122\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_workers \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 123\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__class__\u001b[39;49m\u001b[39m.\u001b[39;49m_mean_average_precision_single_process(queries)\n\u001b[1;32m    124\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    125\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m_mean_average_precision_multi_process(queries)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.14/lib/python3.9/site-packages/pv211_utils/eval.py:133\u001b[0m, in \u001b[0;36mEvaluationBase._mean_average_precision_single_process\u001b[0;34m(cls, queries)\u001b[0m\n\u001b[1;32m    131\u001b[0m average_precisions \u001b[39m=\u001b[39m []\n\u001b[1;32m    132\u001b[0m \u001b[39mfor\u001b[39;00m query \u001b[39min\u001b[39;00m queries:\n\u001b[0;32m--> 133\u001b[0m     precision \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_CURRENT_INSTANCE\u001b[39m.\u001b[39;49m_average_precision_worker(query)\n\u001b[1;32m    134\u001b[0m     \u001b[39mif\u001b[39;00m precision \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    135\u001b[0m         average_precisions\u001b[39m.\u001b[39mappend(precision)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.14/lib/python3.9/site-packages/pv211_utils/eval.py:156\u001b[0m, in \u001b[0;36mEvaluationBase._average_precision_worker\u001b[0;34m(cls, query)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_average_precision_worker\u001b[39m(\u001b[39mcls\u001b[39m, query: QueryBase) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Optional[\u001b[39mfloat\u001b[39m]:\n\u001b[1;32m    155\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_CURRENT_INSTANCE\u001b[39m.\u001b[39msystem\u001b[39m.\u001b[39msearch(query)\n\u001b[0;32m--> 156\u001b[0m     precision \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_CURRENT_INSTANCE\u001b[39m.\u001b[39;49m_average_precision(query, results)\n\u001b[1;32m    157\u001b[0m     \u001b[39mreturn\u001b[39;00m precision\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.14/lib/python3.9/site-packages/pv211_utils/eval.py:98\u001b[0m, in \u001b[0;36mEvaluationBase._average_precision\u001b[0;34m(self, query, results)\u001b[0m\n\u001b[1;32m     96\u001b[0m precision \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m     97\u001b[0m seen_documents \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n\u001b[0;32m---> 98\u001b[0m \u001b[39mfor\u001b[39;00m document_number, document \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(results):\n\u001b[1;32m     99\u001b[0m     \u001b[39mif\u001b[39;00m document \u001b[39min\u001b[39;00m seen_documents:\n\u001b[1;32m    100\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [9], line 31\u001b[0m, in \u001b[0;36mIRSystem.search\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msearch\u001b[39m(\u001b[39mself\u001b[39m, query: Query) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Iterable[Document]:\n\u001b[1;32m     29\u001b[0m     query_doc \u001b[39m=\u001b[39m simple_preprocess(query\u001b[39m.\u001b[39mbody)\n\u001b[1;32m     30\u001b[0m     similarities \u001b[39m=\u001b[39m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbm25_model\u001b[39m.\u001b[39mget_scores(query_doc,\n\u001b[0;32m---> 31\u001b[0m                                                         \u001b[39msum\u001b[39m(\u001b[39mmap\u001b[39m(\u001b[39mlambda\u001b[39;00m k: \u001b[39mfloat\u001b[39m(bm25\u001b[39m.\u001b[39midf[k]), bm25\u001b[39m.\u001b[39;49midf\u001b[39m.\u001b[39mkeys())) \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(bm25\u001b[39m.\u001b[39midf\u001b[39m.\u001b[39mkeys())))\n\u001b[1;32m     32\u001b[0m     similarities \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(similarities, key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m item: item[\u001b[39m1\u001b[39m], reverse\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     33\u001b[0m     ranked_documents \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex_to_document[i] \u001b[39mfor\u001b[39;00m i,_ \u001b[39min\u001b[39;00m similarities)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'gensim.summarization.bm25' has no attribute 'idf'"
     ]
    }
   ],
   "source": [
    "from pv211_utils.beir.eval import BeirEvaluation\n",
    "\n",
    "submit_result = False\n",
    "author_name = 'Surname, Name'\n",
    "system = IRSystem()\n",
    "\n",
    "evaluation = BeirEvaluation(system, test_judgements, None,author_name, num_workers=1)\n",
    "evaluation.evaluate(tqdm(test_queries.values(), desc='Querying the system'), submit_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a297dbfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "TermFrequency-InverseDocumentFrequency_example.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "97b31d6e62de2216a05dd9342162045e53cee058ed98d00a361b193ba69cab9f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
