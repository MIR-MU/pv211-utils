{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5PICy-xBsj7P"
   },
   "source": [
    "# Data Loading\n",
    "\n",
    "The datasets module provides interface for loading Arqmath, Cranfield, Trec, and Beir collection datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kzw_-3Oisj7P"
   },
   "source": [
    "## Cranfield\n",
    "\n",
    "The Cranfield collection consists of 1398 abstracts of aerodynamics journal articles, a set of 225 queries,\n",
    "and exhaustive relevance judgments of all (query, document) pairs.[1, Section 8.2]\n",
    "\n",
    "To load cranfield collection we need to construct `CranfieldDataset` object where we can set how to split the data set into test, train, and validation sets.\n",
    "\n",
    "\n",
    "\n",
    "[1] Manning, Christopher D., Prabhakar Raghavan, and Hinrich Schütze. [*Introduction to information retrieval*](https://nlp.stanford.edu/IR-book/pdf/irbookonlinereading.pdf). Cambridge university press, 2008."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GE0oVdY3sj7P"
   },
   "outputs": [],
   "source": [
    "from pv211_utils.datasets import CranfieldDataset\n",
    "\n",
    "# Creating the dataset object and setting the parameters.\n",
    "cranfield_data = CranfieldDataset(test_split_size=0.2, validation_split_size=0)\n",
    "\n",
    "# The parameters can be changed with methods. For example:\n",
    "cranfield_data.set_validation_split_size(new_size=0.1)\n",
    "\n",
    "# Loading documents.\n",
    "documents = cranfield_data.load_documents()  # -> OrderedDict of {document_id : Document}\n",
    "\n",
    "print(\"document body:\")\n",
    "print(list(documents.values())[0].body)\n",
    "\n",
    "# Examples of loading queries and judgements\n",
    "queries = cranfield_data.load_test_queries() # -> OrderedDict of {query_id : Query}\n",
    "judgements = cranfield_data.load_train_judgements() # -> Set of (Query, Document) pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HlkWjSlksj7P"
   },
   "source": [
    "## Arqmath\n",
    "\n",
    "Arqmath dataset is based on threads from math StackExchange and consists of queries, answers, questions, and relevance judgements between queries and answers. For more information see <a href=\"https://www.cs.rit.edu/~dprl/ARQMath/index.html\">Arqmath web</a>.\n",
    "\n",
    "We can construct `ArqmathDataset` object where we can set how to split the data set into test, train, and validation sets. The test/train split is determined by years (chosen year becomes test set and the remaining two become train set), the validation set is obtained by further splitting the train set. We also need to choose text format, which defines in which format/encoding the text and mathematical formulae in questions, answers, and queries will be.\n",
    "\n",
    "Available years - `2020`, `2021`, `2022`.\n",
    "\n",
    "Available text formats: \n",
    "- `text` - Plain text which does not contain any mathematical formulae.\n",
    "- `text+latex` - Plain text with mathematical formulae in LaTeX surrounded by dollar signs.\n",
    "- `text+prefix` - Plain text with mathematical formulae in [the mathtuples format][5] of [the Tangent-L system][6].\n",
    "- `text+tangentl` - Plain text with mathematical formulae in [the prefix format][1].\n",
    "- `xhtml+latex` - XHTML text with mathematical formulae in LaTeX, surrounded by the `<span class=\"math-container\">` tags.\n",
    "- `xhtml+cmml` - XHTML text with mathematical formulae in the [Presentation MathML][4] XML format.\n",
    "- `xhtml+pmml`- XHTML text with mathematical formulae in the [Content MathML][3] XML format.\n",
    "\n",
    " [1]: http://ceur-ws.org/Vol-2696/paper_235.pdf#page=5\n",
    " [2]: https://en.wikipedia.org/wiki/Polish_notation\n",
    " [3]: https://www.w3.org/TR/MathML2/chapter4.html\n",
    " [4]: https://www.w3.org/TR/MathML2/chapter3.html\n",
    " [5]: https://github.com/fwtompa/mathtuples\n",
    " [6]: http://ceur-ws.org/Vol-2936/paper-05.pdf#page=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "knvipD_xsj7P"
   },
   "outputs": [],
   "source": [
    "from pv211_utils.datasets import ArqmathDataset\n",
    "\n",
    "# Creating the dataset object and setting the parameters.\n",
    "arqmath_data = ArqmathDataset(year=2022, text_format=\"text\", validation_split_size=0.2)\n",
    "\n",
    "# The parameters can be changed with methods. For example:\n",
    "arqmath_data.set_text_format(new_text_format=\"text+latex\")\n",
    "\n",
    "# Loading answers and questions.\n",
    "questions = arqmath_data.load_questions() # -> OrderedDict of {question_id : Question}\n",
    "answers = arqmath_data.load_answers() # -> OrderedDict of {answer_id : Answer}\n",
    "\n",
    "print(\"answer body:\")\n",
    "print(list(answers.values())[0].body)\n",
    "\n",
    "print(\"question body:\")\n",
    "print(list(questions.values())[0].body)\n",
    "\n",
    "# Examples of loading queries and judgements\n",
    "queries = arqmath_data.load_test_queries() # -> OrderedDict of {query_id : Query}\n",
    "judgements = arqmath_data.load_train_judgements() # -> Set of (Query, Document) pairs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "prKWXd4xz2JV"
   },
   "source": [
    "## Trec\n",
    "\"Text Retrieval Conference (TREC). The U.S. National Institute of StandardsTREC\n",
    "and Technology (NIST) has run a large IR test bed evaluation series since\n",
    "1992.[...] TRECs 6–8 provide 150 information needs\n",
    "over about 528,000 newswire and Foreign Broadcast Information Service\n",
    "articles. [...] there are no exhaustive relevance judgments.\" [1, Section 8.2]\n",
    "\n",
    "To load Trec collection we need to construct `TrecDataset` object where we can set how to split the train\n",
    "\n",
    "\n",
    "[1] Manning, Christopher D., Prabhakar Raghavan, and Hinrich Schütze. [*Introduction to information retrieval*](https://nlp.stanford.edu/IR-book/pdf/irbookonlinereading.pdf). Cambridge university press, 2008."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TuA4guHo0ANn"
   },
   "outputs": [],
   "source": [
    "from pv211_utils.datasets import TrecDataset\n",
    "\n",
    "# Creating the dataset object and setting the parameters.\n",
    "trec_data = TrecDataset(validation_split_size=0)\n",
    "\n",
    "# The parameters can be changed with methods. For example:\n",
    "trec_data.set_validation_split_size(new_size=0.2)\n",
    "\n",
    "# Loading documents.\n",
    "documents = trec_data.load_documents() # -> OrderedDict of {document_id : Document}\n",
    "\n",
    "print(\"document body:\")\n",
    "print(list(documents.values())[0].body)\n",
    "\n",
    "# Examples of loading queries and judgements\n",
    "queries = trec_data.load_test_queries() # -> OrderedDict of {query_id : Query}\n",
    "judgements = trec_data.load_train_judgements() # -> Set of (Query, Document) pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q1TpcWptE7t-"
   },
   "source": [
    "## Beir\n",
    "Beir is a benchmark/collection of heterogenous datasets. For more info about the collection or the available datasets see <a href=\"https://github.com/beir-cellar/beir\">Beir github page </a>.\n",
    "\n",
    "To load an dataset from the Beir collection we need to create a `BeirDataset` object and specify the desired dataset's name. The splitting into test/train/validation sets is as provided by the given dataset (see <a href=\"https://github.com/beir-cellar/beir\">Beir github page </a>).\n",
    "\n",
    "Available datasets: \n",
    "- `msmarco` and `msmarco-v2` - A dataset of question from Bing search query logs with human generated answer.\n",
    "- `trec-covid` - A collection of covid related questions and documents from covid-19 open research dataset.\n",
    "- `nfcorpus` - A medical dataset consisting of questions and medical documents.\n",
    "- `nq` -  Questions and wikipedia based documents.\n",
    "- `hotpotqa` - Wikipedia based question-answer pairs.\n",
    "- `fiqa` - Questions and documents from various financial sources.\n",
    "- `arguana` - A corpus of argument and best counterargument pairs.\n",
    "- `webis-touche2020` - A corpus focused on looking for arguments for and against a given topic. \n",
    "- `quora` - A dataset consisting of potential question duplicate pairs.\n",
    "- `dbpedia-entity` - Consists of free text queries and entities (entity search). \n",
    "- `scidocs` - A dataset of scientific documents and indications of their relatedness.\n",
    "- `fever` - A dataset of claims with anotations consisting of their validity and the evidence.\n",
    "- `climate-fever` - A datasets of claims about climate with evidence for or against them from wikipedia articles.\n",
    "- `scifact`- Scientific claims paired with evidence that supports/refutes them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mr7rL78rFGua"
   },
   "outputs": [],
   "source": [
    "from pv211_utils.datasets import BeirDataset\n",
    "\n",
    "# Creating the dataset object and specifying the dataset to be loaded.\n",
    "beir_data = BeirDataset(dataset_name=\"scifact\") \n",
    "\n",
    "# Loading documents.\n",
    "documents = beir_data.load_documents() # -> OrderedDict of {document_id : Document}\n",
    "\n",
    "print(\"document body:\")\n",
    "print(list(documents.values())[0].body)\n",
    "\n",
    "# Examples of loading queries and judgements\n",
    "queries = beir_data.load_test_queries() # -> OrderedDict of {query_id : Query}\n",
    "judgements = beir_data.load_train_judgements() # -> Set of (Query, Document) pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mssgsp6yDSDE"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OUu5b7CsEYUK"
   },
   "source": [
    "## Document preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kzxhmCiRDane"
   },
   "source": [
    "The preprocessing module contains several classes for preprocessing documents, taking string inputs and producing outputs as lists of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z9K6LrPXDZ0u"
   },
   "outputs": [],
   "source": [
    "from pv211_utils.preprocessing import NoneDocPreprocessing, LowerDocPreprocessing\n",
    "\n",
    "text = \"2 Horses jumping over a fence in San-Francisco (město v U.S.A.)\"\n",
    "\n",
    "# NoneDocPreprocessing split input by spaces\n",
    "\n",
    "none_preprocess = NoneDocPreprocessing()\n",
    "print(none_preprocess(text))\n",
    "\n",
    "lower_preprocess = LowerDocPreprocessing()\n",
    "print(lower_preprocess(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hhioTJFGDgQJ"
   },
   "outputs": [],
   "source": [
    "from pv211_utils.preprocessing import SimpleDocPreprocessing\n",
    "\n",
    "# SimpleDocPreprocessing split input by spaces and can remove accentuation or too short/long words\n",
    "\n",
    "preprocess = SimpleDocPreprocessing()\n",
    "preprocess2 = SimpleDocPreprocessing(min_len=0)\n",
    "preprocess3 = SimpleDocPreprocessing(min_len=4, max_len=6, deacc=True)\n",
    "\n",
    "print(preprocess(text))\n",
    "print(preprocess2(text))\n",
    "print(preprocess3(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing and linking nltk\n",
    "!pip3 install nltk\n",
    "import sys\n",
    "import site\n",
    "\n",
    "site.addsitedir(site.USER_SITE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LFyjB6f8DiIT"
   },
   "outputs": [],
   "source": [
    "from pv211_utils.preprocessing import DocPreprocessing\n",
    "from gensim.parsing.porter import PorterStemmer\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "def simple_stem(x):\n",
    "    return x.replace(\"es\", \"\").replace(\"ing\", \"\")\n",
    "\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# DocPreprocessing offers more options, like stemming or lemmatization\n",
    "\n",
    "preprocess = DocPreprocessing(stem=simple_stem, stopwords=['fence'])\n",
    "preprocess2 = DocPreprocessing(stem=stemmer.stem, lemm=lemmatizer.lemmatize)\n",
    "preprocess3 = DocPreprocessing(stem=stemmer.stem)\n",
    "preprocess4 = DocPreprocessing(lemm=lemmatizer.lemmatize)\n",
    "\n",
    "print(preprocess(text))\n",
    "print(preprocess2(text))\n",
    "print(preprocess3(text))\n",
    "print(preprocess4(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8j-RHFuHDkdM"
   },
   "source": [
    "## Math Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0cBpDrRADpsA"
   },
   "source": [
    "It's often a good idea to preprocess plain-text math. Language models like BERT are often trained on Latex representations, so you will achieve the best results with that. If you are trying to do something fancier, like building the syntax trees, you may find it easier to work with MathML representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2bIceCEuDoEG"
   },
   "outputs": [],
   "source": [
    "from pv211_utils.preprocessing import exp_to_latex, exp_to_pmathml, exp_to_cmathml\n",
    "\n",
    "pyexp = \"2**3 + ((4*x)**2) / 8\"\n",
    "\n",
    "print(f\"expression:\\n{pyexp}\")\n",
    "\n",
    "print(f\"\\nlatex:\\n{exp_to_latex(pyexp)}\")\n",
    "\n",
    "print(f\"\\npresentation mathml:\\n{exp_to_pmathml(pyexp)}\")\n",
    "\n",
    "print(f\"\\ncontent mathml:\\n{exp_to_cmathml(pyexp)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imxsqYvDDUD_"
   },
   "source": [
    "# Information Retrieval Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "draPXtpKteP7"
   },
   "source": [
    "There are several systems in pv211_utils. Ranging from traditional systems like BM25 to systems based on Transformer architectures. Feel free to use and explore them, but note that for the state-of-the-art system, you might need to write and tune them from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ACbkQ2bqEG29"
   },
   "outputs": [],
   "source": [
    "from pv211_utils.datasets import CranfieldDataset\n",
    "from pv211_utils.systems import BM25PlusSystem\n",
    "from pv211_utils.preprocessing import NoneDocPreprocessing\n",
    "from pv211_utils.evaluation_metrics import mean_precision\n",
    "\n",
    "cranfield = CranfieldDataset(0.25)\n",
    "\n",
    "judgements = cranfield.load_test_judgements()\n",
    "queries = cranfield.load_test_queries()\n",
    "documents = cranfield.load_documents()\n",
    "\n",
    "preprocessing = NoneDocPreprocessing()\n",
    "bm25 = BM25PlusSystem(documents, preprocessing)\n",
    "result = mean_precision(system=bm25, queries=queries, judgements=judgements, k=5, num_processes=1)\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T1NsDgGezFhG"
   },
   "source": [
    "pv211_utils contains two transformer-based IR systems - retriever and reranker.\n",
    "\n",
    "- Retriever systems compute vector representation for each document, and during a search, they will compare them with query representation using similarity measures like cosine similarity or euclidean distance.\n",
    "- Reranker systems consist of the retriever part, but additionally reranks top-k documents using CrossEncoder (for more see: https://www.sbert.net/examples/applications/cross-encoder/README.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u9FrsVlLrol0"
   },
   "outputs": [],
   "source": [
    "from pv211_utils.datasets import ArqmathDataset\n",
    "from pv211_utils.systems import RetrieverSystem\n",
    "from pv211_utils.evaluation_metrics import mean_average_precision\n",
    "from sentence_transformers.SentenceTransformer import SentenceTransformer\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "arqmath_data = ArqmathDataset(2020, \"text+latex\")\n",
    "\n",
    "answers = arqmath_data.load_answers()\n",
    "# There is 1.4M answers in full ARQMath. Let's use just a small subset for demonstration.\n",
    "answers_subset = dict(list(answers.items())[0:50000])\n",
    "\n",
    "# use pretrained transformer model from hugging face and embed our subset\n",
    "retriever_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "retriever_system = RetrieverSystem(retriever_model, answers_subset)\n",
    "\n",
    "queries = arqmath_data.load_test_queries()\n",
    "judgements = arqmath_data.load_test_judgements()\n",
    "\n",
    "# evaluate retriever system\n",
    "result = mean_average_precision(retriever_system, queries, judgements, 100, 1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QVC1BbXSUPtq"
   },
   "source": [
    "# Evaluation metrics\n",
    "\n",
    "The `evaluation_metrics` module provides functions/metrics for evaluation of IR systems. \n",
    "The metrics included are:\n",
    "- Mean Precision\n",
    "- Mean Recall\n",
    "- Mean Average Precision (MAP)\n",
    "- Normalized Discounted Cumulative Gain (nDCG)\n",
    "- Mean Bpref"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pafqLFlvUghu"
   },
   "source": [
    "## Mean Precision\n",
    "\n",
    "Precision is a fraction of number of relevant retrieved documents in the first k retrieved documets and k, then we take the mean precision over all queries. The formula:\n",
    "\n",
    "$\\text{P}_k = \\frac{\\text{number of relevant documents in top k}}{k}$\n",
    "\n",
    "$\\text{MP}_k = \\frac{1}{|Q|} \\sum_{q \\in Q} \\text{P}_k(q) $\n",
    "\n",
    "Where $Q$ is set of queries.\n",
    "\n",
    "<br> \n",
    "\n",
    "To calculate mean precision of an IR system we call `mean_precision` function :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "judgements = cranfield.load_test_judgements()\n",
    "queries = cranfield.load_test_queries()\n",
    "documents = cranfield.load_documents()\n",
    "\n",
    "preprocessing = NoneDocPreprocessing()\n",
    "bm25 = BM25PlusSystem(documents, preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qurc6fOSUlRH"
   },
   "outputs": [],
   "source": [
    "from pv211_utils.evaluation_metrics import mean_precision \n",
    "\n",
    "mp_score = mean_precision(system=bm25,            # System to be evaluated (must follows IRSystemBase template).\n",
    "                          queries=queries,        # Queries to be used in the evaluation.\n",
    "                          judgements=judgements,  # Judgements to be used in the evaluation.\n",
    "                          k=10,                   # Depth of the evaluation.\n",
    "                          num_processes=4)        # Number of processes/workers to be used to run the evaluation.\n",
    "\n",
    "print(f\"Mean Precision: {mp_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M40LfyeOUoKA"
   },
   "source": [
    "## Mean Recall\n",
    "\n",
    "Recall is a fraction of number of relevant retrieved documents in the first k retrieved documets and number of relevant documents, then we take the mean recall over all queries. The formula:\n",
    "\n",
    "$\\text{R}_k = \\frac{\\text{number of relevant documents in top k}}{\\text{mumber of relevant documents}}$\n",
    "\n",
    "$\\text{MR}_k = \\frac{1}{|Q|} \\sum_{q \\in Q} \\text{R}_k(q) $\n",
    "\n",
    "Where $Q$ is set of queries.\n",
    "\n",
    "<br> \n",
    "\n",
    "To calculate mean recall of an IR system we call `mean_recall` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aTIPAT6VUqKZ"
   },
   "outputs": [],
   "source": [
    "from pv211_utils.evaluation_metrics import mean_recall \n",
    "\n",
    "mr_score = mean_recall(system=bm25,            # System to be evaluated (must follows IRSystemBase template).\n",
    "                       queries=queries,        # Queries to be used in the evaluation.\n",
    "                       judgements=judgements,  # Judgements to be used in the evaluation.\n",
    "                       k=10,                   # Depth of the evaluation.\n",
    "                       num_processes=4)        # Number of processes/workers to be used to run the evaluation.\n",
    "\n",
    "print(f\"Mean Recall: {mr_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Ib3r-xyUbRQ"
   },
   "source": [
    "## MAP \n",
    "\n",
    "MAP is claculated by firstly calculating an average precision (AP) at k (we only take first k documents returned into consideration) for each query, then we calculate the mean of these APs. The average precision assigns less value to the relevant documents that are lower in the ranked list of retrieved documents than to those in higher positions. The formula:\n",
    "\n",
    "$\\text{AP}_k = \\frac{1}{\\text{number of relevant documents in top k}} * \\sum_{i = 1}^{k} \\text{P}_i * r_i$\n",
    "\n",
    "$\\text{MAP}_k = \\frac{1}{|Q|} \\sum_{q \\in Q} \\text{AP}_k(q)$\n",
    "\n",
    "Where $\\text{P}_i$ is precision at i, $r_i$ is indicator of relevance of i-th document, and $Q$ is set of queries.\n",
    "\n",
    "<br> \n",
    "\n",
    "To calculate MAP of an IR system we call `mean_average_precision` function :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YG9nUJyVUb2w"
   },
   "outputs": [],
   "source": [
    "from pv211_utils.evaluation_metrics import mean_average_precision\n",
    "\n",
    "map_score = mean_average_precision(system=bm25,            # System to be evaluated (must follows IRSystemBase template).\n",
    "                                   queries=queries,        # Queries to be used in the evaluation.\n",
    "                                   judgements=judgements,  # Judgements to be used in the evaluation.\n",
    "                                   k=10,                   # Depth of the evaluation.\n",
    "                                   num_processes=4)        # Number of processes/workers to be used to run the evaluation.\n",
    "\n",
    "print(f\"MAP: {map_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "moz6542ZUtnP"
   },
   "source": [
    "## Mean nDCG\n",
    "\n",
    "nDCG is calculated as fraction of discounted cumulative gain (DCG) and ideal discounted cumulative gain (IDCG). The DCG calculates the cumulative \n",
    "gain of relevance of document in the retrieved document list, where the document's relevance value is penalized (discounted) for being lower in the result.\n",
    "The IDCG represents the maximum possible DCG for given query and is used to normalize the DCG. This normalization is done because the value of DCG also depends on the list's size or more specifically, the total number of relevant documents in the result list. The formula:\n",
    "\n",
    "$\\text{DCG}_k = \\sum_{i=1}^{k}\\frac{r_i}{\\log_{2}(i + 1)}$\n",
    "\n",
    "$\\text{IDCG}_k = \\sum_{i=1}^{|\\text{rel}_k|}\\frac{\\text{rel}_k[i]}{\\log_{2}(i + 1)}$\n",
    "\n",
    "$\\text{mean_nDCG}_k = \\frac{1}{|Q|} \\sum_{q \\in Q} \\frac{\\text{DCG}_k(q)}{\\text{IDCG}_k(q)}$\n",
    "\n",
    "Where $r_i$ is an indicator of relevance of the i-th document, $\\text{rel}_k$ is a list of relevant documents sorted by their relevance up to position k,<br>\n",
    " $\\text{rel}_k[i]$ is a relevance of the i-th document in $\\text{rel}_k$ list, and $Q$ is set of queries.\n",
    "\n",
    "<br> \n",
    "\n",
    "To calculate mean nDCG of an IR system we call `normalized_discounted_cumulative_gain` function :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jn0_prSoUwfn"
   },
   "outputs": [],
   "source": [
    "from pv211_utils.evaluation_metrics import normalized_discounted_cumulative_gain \n",
    "\n",
    "ndcg_score = normalized_discounted_cumulative_gain(system=bm25,            # System to be evaluated (must follows IRSystemBase template).\n",
    "                                                   queries=queries,        # Queries to be used in the evaluation.\n",
    "                                                   judgements=judgements,  # Judgements to be used in the evaluation.\n",
    "                                                   k=10,                   # Depth of the evaluation.\n",
    "                                                   num_processes=4)        # Number of processes/workers to be used to run the evaluation.\n",
    "\n",
    "print(f\"nDCG: {ndcg_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qZtuQkCWUz2-"
   },
   "source": [
    "## Mean Bpref\n",
    "\n",
    "\"Bpref is a preference-based information retrieval measure that considers whether relevant documents are ranked above irrelevant ones.<br>\n",
    " It is designed to be robust to missing relevance judgments, such that it gives the same experimental outcome with incomplete judgments <br> that Mean Average Precision would with complete judgments.\"[2]\n",
    "The formula:\n",
    "\n",
    "$\\text{Bpref}_k = \\frac{1}{|R|} \\sum_{r \\in R} 1 - \\frac{\\text{|n ranked higher than r|}}{|R|}$\n",
    "\n",
    "$\\text{mean_Bpref}_k = \\frac{1}{|Q|} \\sum_{q \\in Q} \\text{Bpref}_k(q)$\n",
    "\n",
    "Where $R$ is set of relevant documents from the top k documents in result list, n is a nonrelevant document from first $|R|$ retrieved nonrelevant documents, and $Q$ is set of queries.\n",
    "\n",
    "<br>\n",
    "\n",
    "To calculate mean mean Bpref of an IR system we call `mean_bpref` function:\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "[2]Craswell, N. (2009). Bpref. In: LIU, L., ÖZSU, M.T. (eds) Encyclopedia of Database Systems. Springer, Boston, MA. https://doi.org/10.1007/978-0-387-39940-9_489"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_UWLEosmU2Af"
   },
   "outputs": [],
   "source": [
    "from pv211_utils.evaluation_metrics import mean_bpref \n",
    "\n",
    "bpref_score = mean_bpref(system=bm25,            # System to be evaluated (must follows IRSystemBase template).\n",
    "                         queries=queries,        # Queries to be used in the evaluation.\n",
    "                         judgements=judgements,  # Judgements to be used in the evaluation.\n",
    "                         k=10,                   # Depth of the evaluation.\n",
    "                         num_processes=4)        # Number of processes/workers to be used to run the evaluation.\n",
    "\n",
    "print(f\"Bpref: {bpref_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TMXk4vYEVRfH"
   },
   "source": [
    "# Ensembling Algorithms\n",
    "\n",
    "The `ensembles` module provides functions for ensambling of IR systems. These included methods based on:\n",
    "- inverse mean rank\n",
    "- inverse median rank\n",
    "- reciprocal rank fusion\n",
    "- IBC\n",
    "- weighted IBC\n",
    "- RBC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wLWfmfzLpc99"
   },
   "source": [
    "An example of how to create an ensamble IR system using inverse mean rank:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AxtLdheCpeEB"
   },
   "outputs": [],
   "source": [
    "# importing an ensamble algorithm\n",
    "from pv211_utils.ensembles import inverse_mean_rank\n",
    "# other imports\n",
    "from pv211_utils.systems import BM25PlusSystem, TfidfSystem\n",
    "from pv211_utils.datasets import CranfieldDataset\n",
    "from pv211_utils.preprocessing import NoneDocPreprocessing\n",
    "from pv211_utils.irsystem import IRSystemBase\n",
    "from pv211_utils.evaluation_metrics import mean_average_precision\n",
    " \n",
    "# Create the systems to be ensambled and load data\n",
    "data = CranfieldDataset()\n",
    "system_1 = BM25PlusSystem(data.load_documents(), NoneDocPreprocessing())\n",
    "system_2 = TfidfSystem(data.load_documents(), NoneDocPreprocessing())\n",
    "\n",
    "# Create ensamble IR system\n",
    "class EnsambleSystem(IRSystemBase):\n",
    "    def __init__(self, systems):\n",
    "        self.systems = systems\n",
    "\n",
    "    def search(self, query):\n",
    "        return inverse_mean_rank(query, self.systems)\n",
    "\n",
    "# Ensambling the system_1 and system_2\n",
    "ens_system = EnsambleSystem([system_1, system_2])\n",
    "\n",
    "# We can evaluate its MAP score and compare it to individual systems' scores\n",
    "print(f\"BM25 system's MAP: {mean_average_precision(system_1, data.load_test_queries(), data.load_test_judgements(), 10, 4)}\")\n",
    "print(f\"TF-IDF system's MAP: {mean_average_precision(system_2, data.load_test_queries(), data.load_test_judgements(), 10, 4)}\")\n",
    "print(f\"ensamble system's MAP: {mean_average_precision(ens_system, data.load_test_queries(), data.load_test_judgements(), 10, 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J6Jc3-Tmpi7O"
   },
   "source": [
    "## Inverse Mean Rank\n",
    "\n",
    "The inverse mean rank ensembling algorithm takes list of IR systems and a query and produces a list of documents ranked by they inverse mean rank. The formula for inverse mean rank of document $i$:\n",
    "\n",
    "$\\text{inverse_mean_rank}_i = \\frac{1}{\\text{mean}(\\text{ranks}_i)}$\n",
    "\n",
    "Where $\\text{ranks}_i$ is a list of ranks of document $i$ in the results of individual systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HzvpZUOPpjPR"
   },
   "source": [
    "## Inverse Median Rank\n",
    "\n",
    "The inverse median rank ensembling algorithm takes list of IR systems and a query and produces a list of documents ranked by they inverse median rank. The formula for inverse median rank of document $i$:\n",
    "\n",
    "$\\text{inverse_median_rank}_i = \\frac{1}{\\text{median}(\\text{ranks}_i)}$\n",
    "\n",
    "Where $\\text{ranks}_i$ is a list of ranks of document $i$ in the results of individual systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r7Irc6f9pjiz"
   },
   "source": [
    "## Reciprocal Rank Fusion\n",
    "\n",
    "The reciprocal rank fusion (RRF) ensembling algorithm takes list of IR systems, a query, and a parameter $k$ and produces a list of documents ranked by RRF score, where for document $i$ the formula is: \n",
    "\n",
    "$\\text{RRF_score}_i = \\sum_{s \\in S}\\frac{1}{k + \\text{rank}_i(s)}$\n",
    "\n",
    "Where $S$ is the set of the ensambled systems, $\\text{rank}_i(s)$ is the rank of document $i$ in system's $s$ result, and $k$ is a parameter.\n",
    "\n",
    "The authors'reason behind the constant $k$ in the formula is that it should mitigate the impact of high ranks given by outlier systems.[1]\n",
    "\n",
    "[1] Gordon V. Cormack, Charles L A Clarke, and Stefan Buettcher. 2009. Reciprocal rank fusion outperforms condorcet and individual rank learning methods. In Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval (SIGIR '09). Association for Computing Machinery, New York, NY, USA, 758–759. https://doi.org/10.1145/1571941.1572114"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "huYTNPDvpj0Q"
   },
   "source": [
    "## IBC\n",
    "\n",
    "The IBC ensembling algorithm takes list of IR systems and a query and produces a list of documents ranked by score given by formula (for document $i$):\n",
    "\n",
    "$\\text{ibc}_i = \\frac{|D| - \\text{median}(\\text{ranks}_i)}{|D|}$\n",
    "\n",
    "Where $\\text{ranks}_i$ is a list of ranks of document $i$ in the results of individual systems, and D is set of all documents.\n",
    "\n",
    "The IBC also incorporates a tie braking mechanism, where ties between documents are broken by calculating new scores from rank selected uniformly at random from their lists of ranks (instead of taking median rank). Further ties are broken randomly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vXCws882pkFc"
   },
   "source": [
    "## Weighted IBC\n",
    "\n",
    "The Weighted IBC (WIBC) ensembling algorithm takes list of IR systems, a query, and a weights of systems and produces a list of documents ranked by score given by formula (for document $i$):\n",
    "\n",
    "$\\text{wibc}_i = \\frac{|D| - \\text{weighted_median}(\\text{ranks}_i, \\text{ weights})}{|D|}$\n",
    "\n",
    "Where $\\text{ranks}_i$ is a list of ranks of document $i$ in the results of individual systems, weights is list of systems' weights, and D is set of all documents.\n",
    "\n",
    "The WIBC also incorporates a tie braking mechanism, where ties between documents are broken by calculating new scores from rank selected at random (distribution is defined by weights) from their lists of ranks (instead of taking median rank). Further ties are broken randomly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "inG2kbYEqMQU"
   },
   "source": [
    "## RBC\n",
    "\n",
    "The RBC algorithm uses a trained model (linear regression by default) to estimate relevance of a document based on its ranks in result lists of the individual systems (more specifically the score used to train/predict is calculated as $\\frac{|D| - \\text{rank}}{|D|}$, where $D$ is set of all documents). The result document list is sorted by this predicted relevance.\n",
    "\n",
    "To create RBC ensamble we create an object of rbc class, where the parameters are systems to be ensambled, queries and judgements used for trainign the model, and optionally a pipeline (the default is standard scaler and linear regression model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WEqrs-ilqO6p"
   },
   "outputs": [],
   "source": [
    "# Import the rbc class.\n",
    "from pv211_utils.ensembles import Rbc\n",
    "# Other imports.\n",
    "from pv211_utils.systems import BM25PlusSystem, TfidfSystem\n",
    "from pv211_utils.datasets import CranfieldDataset \n",
    "from pv211_utils.preprocessing import DocPreprocessing \n",
    "from pv211_utils.evaluation_metrics import mean_average_precision\n",
    " \n",
    "# Create the systems to be ensambled and load data.\n",
    "data = CranfieldDataset(0.1)\n",
    "system_1 = BM25PlusSystem(data.load_documents(), DocPreprocessing())\n",
    "system_2 = TfidfSystem(data.load_documents(), DocPreprocessing())\n",
    "\n",
    "# Create an ensamble system.\n",
    "rbc_ens = Rbc([system_1, system_2], # Systems to be ensambled.\n",
    "              data.load_train_queries(), # Queries used for training.\n",
    "              data.load_train_judgements()) # Judgements used for training.\n",
    "\n",
    "\n",
    "# We can evaluate its MAP score and compare it to individual systems' scores\n",
    "print(f\"BM25 system's MAP: {mean_average_precision(system_1, data.load_test_queries(), data.load_test_judgements(), 10, 4)}\")\n",
    "print(f\"TF-IDF system's MAP: {mean_average_precision(system_2, data.load_test_queries(), data.load_test_judgements(), 10, 4)}\")\n",
    "print(f\"RBC ensamble system's MAP: {mean_average_precision(rbc_ens, data.load_test_queries(), data.load_test_judgements(), 10, 4)}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "5PICy-xBsj7P",
    "QVC1BbXSUPtq"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "97b31d6e62de2216a05dd9342162045e53cee058ed98d00a361b193ba69cab9f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
